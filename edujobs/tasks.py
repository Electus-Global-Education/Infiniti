# edujob/tasks.py
import time
from celery import shared_task
from core.utils import generate_gemini_response, ALLOWED_MODELS, DEFAULT_MODEL, DEFAULT_TEMPERATURE

@shared_task(bind=True)
def generate_edujob_chat_task(self, prompt: str, model_name: str, temperature: float):
    """
     Asynchronous Celery task to generate a chat response using an Gemini.
    
    Steps performed:
    1. Validate and apply default values for the model and temperature if needed.
    2. Measure how long the LLM call takes.
    3. Call the LLM utility to get the generated response.
    4. Return a structured dictionary containing the response text, used model, temperature, and elapsed time.
    """
     # Step 1: Validate and apply defaults
    # - Use provided model_name only if it's in the allowed list; otherwise fallback to DEFAULT_MODEL.
    # - Use the given temperature if it's not None; otherwise use DEFAULT_TEMPERATURE.
    # enforce defaults
    model = model_name if model_name in ALLOWED_MODELS else DEFAULT_MODEL
    temp = temperature if temperature is not None else DEFAULT_TEMPERATURE

    # Step 2: Start timing to measure the duration of the LLM call
    start = time.perf_counter()

    # Step 3: Call the LLM via your helper function (generate_gemini_response)
    # This sends the prompt to the LLM and receives the generated text.
    result = generate_gemini_response(prompt=prompt, model_name=model, temperature=temp)

    # Step 4: Calculate the elapsed time of the LLM call
    elapsed = time.perf_counter() - start

    # Step 5: Return a structured dictionary:
    # - "response": the text generated by the model
    # - "model": which model was actually used
    # - "temperature": the final temperature applied
    # - "elapsed": time taken for the LLM call in seconds, rounded to 4 decimal places
    return {
        "response": result["response"],
        "model": model,
        "temperature": temp,
        "elapsed":    round(elapsed, 4),
    }
